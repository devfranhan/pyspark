{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc708f3a-fcae-4f51-a5ca-bfb1af91fa0c",
     "showTitle": true,
     "title": "libs"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window as wd, Row\n",
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "521f9c51-4867-4ea3-b1dc-c6e8f9c2b5c3",
     "showTitle": true,
     "title": "keys"
    }
   },
   "outputs": [],
   "source": [
    "def key(fieldlist):\n",
    "    # Create a window specification partitioned by the given fields\n",
    "    window = wd.partitionBy(\n",
    "        *fieldlist\n",
    "    ).orderBy(\n",
    "        # Order the partitions by the specified columns in descending order\n",
    "        F.desc(F.col(\"example_field_1\")), F.desc(F.col(\"example_field_2\"))\n",
    "    )\n",
    "    return window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5236b80e-3f44-4548-9a8c-4262360c1213",
     "showTitle": true,
     "title": "bronze"
    }
   },
   "outputs": [],
   "source": [
    "class bronze:\n",
    "    \n",
    "    @staticmethod\n",
    "    def crm():\n",
    "        # Read data from the specified table with a limit of 100 rows\n",
    "        df = (spark.read\n",
    "                    .table(\"example_database.example_table\")\n",
    "                    .limit(100)\n",
    "                    # Filters\n",
    "                    .filter(F.col(\"example_column\") == \"example_value\")\n",
    "                    # Select specific columns\n",
    "                    .select(\n",
    "                        [\"example_col1\", \"example_col2\", \"example_col3\", \"example_col4\"]\n",
    "                    )\n",
    "                    # Add a column with the current timestamp\n",
    "                    .withColumn(\n",
    "                        \"ingest_time\", F.current_timestamp()\n",
    "                    )\n",
    "        )\n",
    "        # Create or replace a temporary view with the DataFrame\n",
    "        df.createOrReplaceTempView(\"crm_bronze\")\n",
    "\n",
    "    @staticmethod\n",
    "    def api():\n",
    "        # Example JSON data\n",
    "        json_raw = [{\"email\": \"example1@example.com\"}, {\"email\": \"example2@example.com\"}]\n",
    "        # Create DataFrame from JSON data\n",
    "        df = spark.createDataFrame(json_raw)\n",
    "        # Create or replace a temporary view with the DataFrame\n",
    "        df.createOrReplaceTempView(\"bronze_api\")\n",
    "\n",
    "    @staticmethod\n",
    "    def file():\n",
    "        # Read CSV file with specified options\n",
    "        file = (spark.read.format(\"csv\")\n",
    "                .option(\"header\", \"true\")\n",
    "                .option(\"delimiter\", \";\")\n",
    "                .load(\"/FileStore/EXAMPLE_FILE.csv\"))\n",
    "        \n",
    "        # Example new rows to be added to the DataFrame\n",
    "        json_raw = [\n",
    "            {\"example_col1\": \"example_value1\", \"example_col2\": \"example_value2\", \"example_col3\": \"example_value3\"},\n",
    "            {\"example_col1\": \"example_value4\", \"example_col2\": \"example_value5\", \"example_col3\": \"example_value6\"}\n",
    "        ]\n",
    "        # Create DataFrame from new rows\n",
    "        newRows = spark.createDataFrame(json_raw)\n",
    "        # Union the new rows with the original DataFrame\n",
    "        df = file.union(newRows)\n",
    "        # Create or replace a temporary view with the DataFrame\n",
    "        df.createOrReplaceTempView(\"bronze_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbf2ebba-42ad-46ca-acaf-3a333714f91f",
     "showTitle": true,
     "title": "silver"
    }
   },
   "outputs": [],
   "source": [
    "class silver:\n",
    "    \n",
    "    @staticmethod\n",
    "    def transformation():\n",
    "        # Define the window specification for deduplication\n",
    "        rowDedup = key([\"example_email_column\"])\n",
    "\n",
    "        # Read the bronze table and perform transformations\n",
    "        df = (\n",
    "            spark.read.table(\"crm_bronze\")\n",
    "            .withColumn(\n",
    "                # Reverse and format the CNPJ column\n",
    "                \"nr_cnpj_estb\", F.reverse(F.substring(F.concat(F.reverse(F.col(\"nr_cnpj_estb\")), F.lit(\"00000000000000\")), 0, 14))\n",
    "            )\n",
    "            .withColumn(\n",
    "                # Trim the agency code column\n",
    "                \"cd_agen_estb\", F.trim(F.col(\"cd_agen_estb\"))\n",
    "            )\n",
    "            .withColumn(\n",
    "                # Trim the account number column\n",
    "                \"nr_cnta_crrt_estb\", F.trim(F.col(\"nr_cnta_crrt_estb\"))\n",
    "            )\n",
    "            .withColumn(\n",
    "                # Standardize email format and filter invalid emails\n",
    "                \"nm_email_1\", F.when(\n",
    "                    (F.lower(F.col(\"nm_email_1\")).contains(\"@\")) &\n",
    "                    (\n",
    "                        (F.lower(F.col(\"nm_email_1\")).like(\"%__@___%.com.__\")) |\n",
    "                        (F.lower(F.col(\"nm_email_1\")).like(\"%.com.br\"))\n",
    "                    ),\n",
    "                    F.lower(F.col(\"nm_email_1\"))\n",
    "                )\n",
    "            )\n",
    "            .withColumn(\n",
    "                # Add a row number column for deduplication\n",
    "                \"rowDedup\", F.row_number().over(rowDedup)\n",
    "            )\n",
    "            .select(\n",
    "                # Select and alias columns\n",
    "                F.col(\"nr_cnpj_estb\").alias(\"cnpj\"),\n",
    "                F.col(\"nm_email_1\").alias(\"email\"),\n",
    "                F.col(\"cd_agen_estb\").alias(\"agencia\"),\n",
    "                F.col(\"nr_cnta_crrt_estb\").alias(\"conta\"),\n",
    "                F.col(\"data_ref_carga\"),\n",
    "                F.col(\"ingest_time\")\n",
    "            )\n",
    "            .filter(\n",
    "                # Filter rows to keep only the first occurrence per group\n",
    "                F.col(\"rowDedup\") == 1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Read and deduplicate API data\n",
    "        df_api = spark.read.table(\"bronze_api\").dropDuplicates([\"email\"])\n",
    "        # Read and deduplicate file data\n",
    "        df_file = spark.read.table(\"bronze_file\").dropDuplicates([\"CPFCNPJ\"])\n",
    "\n",
    "        # Join the DataFrames\n",
    "        df_matches = (\n",
    "            df.alias(\"a\")\n",
    "            .join(df_api.alias(\"b\"), df.email == df_api.email, \"left\")\n",
    "            .join(df_file.alias(\"c\"), F.expr(\"a.cnpj like concat('%', c.CPFCNPJ, '%')\"), \"left\")\n",
    "            .withColumn(\n",
    "                # Mark matches from API\n",
    "                \"match_api\", F.when(F.col(\"b.email\").isNotNull(), 1).otherwise(0)\n",
    "            )\n",
    "            .withColumn(\n",
    "                # Mark matches from file\n",
    "                \"match_file\", F.when(F.col(\"c.CPFCNPJ\").isNotNull(), 1).otherwise(0)\n",
    "            )\n",
    "            .select(\n",
    "                \"a.*\",\n",
    "                F.col(\"match_api\"),\n",
    "                F.col(\"match_file\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Observability: Print counts to verify the transformations\n",
    "        print(\"Count (original): {}\".format(df.count()))\n",
    "        print(\"Count (transform): {}\".format(df_matches.count()))\n",
    "\n",
    "        if df.count() != df_matches.count():\n",
    "            print(\"Join key duplication error.\")\n",
    "        \n",
    "        # Create or replace a temporary view with the transformed DataFrame\n",
    "        df_matches.createOrReplaceTempView(\"crm_silver\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "medallion",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
